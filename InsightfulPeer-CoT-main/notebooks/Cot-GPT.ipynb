{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb45d513",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-09T10:37:21.446689Z",
     "iopub.status.busy": "2024-10-09T10:37:21.446235Z",
     "iopub.status.idle": "2024-10-09T10:37:36.110058Z",
     "shell.execute_reply": "2024-10-09T10:37:36.108724Z"
    },
    "papermill": {
     "duration": 14.670737,
     "end_time": "2024-10-09T10:37:36.112802",
     "exception": false,
     "start_time": "2024-10-09T10:37:21.442065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\r\n",
      "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.4.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\r\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\r\n",
      "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.9.2)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai) (4.12.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\r\n",
      "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: jiter, openai\r\n",
      "Successfully installed jiter-0.6.1 openai-1.51.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede5837",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T10:37:36.120834Z",
     "iopub.status.busy": "2024-10-09T10:37:36.120420Z",
     "iopub.status.idle": "2024-10-09T12:23:26.821731Z",
     "shell.execute_reply": "2024-10-09T12:23:26.820227Z"
    },
    "papermill": {
     "duration": 6350.70929,
     "end_time": "2024-10-09T12:23:26.825269",
     "exception": false,
     "start_time": "2024-10-09T10:37:36.115979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = '<your openai api key>'\n",
    "\n",
    "def make_api_call(prompt):\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4\", \n",
    "            messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"You are an expert AI assistant tasked with evaluating the completeness and thoroughness of a review. Your goal is to classify reviews as either 'Exhaustive' or 'Trivial' based on their coverage of key sections and aspects. Here are the definitions of the terms:\\n\\n\"\n",
    "\n",
    "                            \"1. **Exhaustive**: The review provides comprehensive feedback across multiple sections and aspects of the paper, offering detailed insight into key areas such as methodology, results, experiments, and more. A review should be classified as 'Exhaustive' if it covers a wide range of sections and aspects (e.g., Abstract, Introduction, Methodology, etc.) with depth, leaving no significant sections or questions unaddressed.\\n\\n\"\n",
    "\n",
    "                            \"2. **Trivial**: The review lacks depth and does not sufficiently cover critical sections or aspects. It might focus only on one or two areas (e.g., comments on Abstract or Introduction) and fails to address significant sections or aspects in detail. A 'Trivial' review might provide shallow or vague comments that do not contribute much to improving the paper.\\n\\n\"\n",
    "\n",
    "                            \"Here are the key sections and aspects you should be aware of:\\n\"\n",
    "                            \"Sections: Abstract (ABS), Introduction (INT), Related Works (RWK), Problem Definition/Idea (PDI), Data/Datasets (DAT), Methodology (MET), Experiments (EXP), Results (RES), Tables & Figures (TNF), Analysis (ANA), Future Work (FWK), Overall (OAL), Bibliography (BIB), External Knowledge (EXT).\\n\"\n",
    "                            \"Aspects: Appropriateness (APR), Originality/Novelty (NOV), Significance/Impact (IMP), Meaningful Comparison (CMP), Presentation/Formatting (PNF), Recommendation (REC), Empirical/Theoretical Soundness (EMP), Substance (SUB), Clarity (CLA).\\n\\n\"\n",
    "\n",
    "                            \"For example, a review that comments on several sections like 'Methodology,' 'Experiments,' and 'Results' in detail and provides constructive feedback on 'Originality,' 'Significance,' and 'Empirical Soundness' should be considered exhaustive. A review that only comments on the 'Introduction' or 'Abstract' without providing much insight into other sections should be considered trivial.\\n\\n\"\n",
    "\n",
    "                            \"### Important:\\n\"\n",
    "                            \"**Your decision MUST be directly supported by the step-by-step reasoning in the CoT (Chain of Thought).** \"\n",
    "                            \"Carefully evaluate each section and aspect mentioned in the CoT reasoning before making your final decision.\\n\\n\"\n",
    "\n",
    "                            \"### Double-Check:\\n\"\n",
    "                            \"Before you finalize your decision, ask yourself: 'Does the reasoning I've provided support an 'Exhaustive' or 'Trivial' decision?' \"\n",
    "                            \"Your decision **must align** with the CoT reasoning.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": (\n",
    "                            f\"{prompt}\\n\\n\"\n",
    "                            \"Evaluate the review's coverage of sections and aspects based on the reasoning provided. \"\n",
    "                            \"Please ensure the chain of thought reasoning is **step-wise**, following the Chain of Thought (CoT) reasoning process.\"\n",
    "                        )\n",
    "                    }\n",
    "                ],\n",
    "            max_tokens=3000,  \n",
    "            temperature=0.2\n",
    "        )\n",
    "        result = response.choices[0].message.content\n",
    "        decision = \"Trivial\" if \"Trivial\" in result else \"Exhaustive\"\n",
    "        return decision, result\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return \"Error\", str(e)\n",
    "\n",
    "def generate_prompt(group):\n",
    "    review_texts = ' '.join(group['review_text'].dropna())\n",
    "    section_coverage = ' '.join(group.filter(like='section_coverage').fillna('').agg(' '.join, axis=1))\n",
    "    aspect_coverage = ' '.join(group.filter(like='aspect_coverage').fillna('').agg(' '.join, axis=1))\n",
    "    return f\"Review Text: {review_texts} Section Coverage: {section_coverage} Aspect Coverage: {aspect_coverage}\"\n",
    "\n",
    "def annotate_dataset(df):\n",
    "    df['Decision'] = None\n",
    "    df['CoT_Reasoning'] = None\n",
    "\n",
    "    grouped = df.groupby(['review_id', 'review_number'])\n",
    "\n",
    "    for (review_id, review_number), group in grouped:\n",
    "        prompt = generate_prompt(group)\n",
    "        decision, cot_reasoning = make_api_call(prompt)\n",
    "\n",
    "        df.loc[(df['review_id'] == review_id) & (df['review_number'] == review_number), 'Decision'] = decision\n",
    "        df.loc[(df['review_id'] == review_id) & (df['review_number'] == review_number), 'CoT_Reasoning'] = cot_reasoning\n",
    "    \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return df\n",
    "df = pd.read_csv('/kaggle/input/newversion/split_dataset_part_4.csv')\n",
    "annotated_df = annotate_dataset(df)\n",
    "annotated_df.to_csv('/kaggle/working/GPT_4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5777208,
     "sourceId": 9494455,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6369.555187,
   "end_time": "2024-10-09T12:23:27.559206",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-09T10:37:18.004019",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
